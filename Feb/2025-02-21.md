## 날짜: 2025-02-21

## 퍼셉트론의 작동 원리와 이진 분류 문제 해결 과정을 설명하시오.
## 퍼셉트론의 작동 원리
### 퍼셉트론이란 무엇인가?

퍼셉트론은 신경망의 기원이 되는 알고리즘으로, 다수의 신호 (`x1, x2, ...`)를 입력으로 받아 하나의 신호(`out(t)`)를 출력한다.   
각 입력 신호에 고유한 가중치(`w1, w2, ...`)를 부여하며, 이는 각 신호가 결과에 주는 영향력을 조절하는 요소로 작용한다.   
아래 그림을 보면, 입력값들이 가중치를 부여받고 net 함수와 활성화 함수를 거쳐 출력값으로 나오는 모습을 볼 수 있다.

![퍼셉트론의 동작 과정](https://blog.kakaocdn.net/dn/biE5T3/btqB3qPkNmb/ZfUn6Xo2Np6kJutwdKQO6k/img.png)

---
### 퍼셉트론의 활성화 함수
활성화 함수는 입력 신호의 계산 결과를 기반으로 출력을 활성화할지 여부를 결정하는 역할을 한다.   

많은 활성화 함수가 있으며 교재에서는 시그모이드 함수와 렐루를 사용하였다
- 시그모이드 함수와 렐루
![시그모이드 함수와 렐루](https://wikidocs.net/images/page/163752/Fig_6.png)
---
### 퍼셉트론의 학습 규칙
퍼셉트론의 학습 과정은 다음과 같이 이루어진다.
1. 초기 가중치를 설정한다.

2. 특정 임계값을 정한다.

3. 입력값과 목표값을 적용하여 퍼셉트론을 활성화한다.

4. 가중치와 입력값의 총합을 활성화 함수에 넣어 출력값을 계산한다.

5. 출력값과 목표값의 오차를 계산한다.

6. 오차를 줄이기 위해 가중치를 업데이트한다.

7. 허용 오차 범위 내에 들어올 때까지 이 과정을 반복한다.

이러한 과정을 통해 최적의 가중치를 찾아가는 것이 퍼셉트론 학습의 핵심이다.

![학습규칙](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FOry5g%2FbtqB4ZqeTos%2FNHE7AtmBFeGoJ4MkkLFLMK%2Fimg.png)

### 퍼셉트론의 학습 방법
퍼셉트론은 모든 학습 데이터를 정확히 분류시킬 때까지 학습이 진행되기 때문에 학습 데이터가 선형적으로 분리될 수 있을 때 적합한 알고리즘이다. 선형분류는 아래와 같이 선으로 분류하는 것을 의미한다. 학습이 반복될수록 선의 기울기가 달라지는 것을 볼 수 있다. 학습을 하면서 weight가 계속 조정(adjust)되는 것이다.
![개 고양이 분류](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Perceptron_example.svg/500px-Perceptron_example.svg.png)

#### Question
가중치를 초기화 할시 왜 아래와 같이 랜덤한 값을 주는걸까?
```python
    self.weights = np.random.rand(input_size)
    self.bias = np.random.rand(1)
```
#### Answer
1. 가중치를 0으로 초기화하면 모든 뉴런이 동일한 값으로 학습됨
- 뉴런의 역할: 입력 데이터를 보고 학습하여 서로 다른 특징을 잡아냄
- 만약 모든 가중치가 0이면, 뉴런들이 같은 방식으로 업데이트됨 → 학습이 의미 없어짐
- 결과적으로 모든 뉴런이 같은 기능을 수행하여, 여러 개의 뉴런이 있어도 하나의 뉴런과 차이가 없어짐

2. 대칭 깨기(Symmetry Breaking) 문제
- 신경망에서는 여러 뉴런이 각기 다른 패턴을 학습해야 함
- 가중치를 0으로 설정하면 모든 뉴런이 동일한 값을 출력하고, 동일한 방식으로 업데이트됨
- 즉, 뉴런들 사이에 차이가 없어져서 학습이 진행되지 않음

3. 랜덤 초기화로 뉴런들이 서로 다른 방향으로 학습
- 가중치를 랜덤하게 초기화하면 각 뉴런이 다른 방향으로 업데이트됨
- 이렇게 하면 뉴런들이 서로 다른 패턴을 학습할 수 있어 학습이 제대로 진행됨

4. 바이어스는 0으로 초기화해도 괜찮음
- 바이어스는 입력과 곱해지지 않고 단순한 상수값이므로, 초기값을 0으로 설정해도 대칭 문제가 발생하지 않음

--- 

## 이진분류 문제 해결 과정
### 이진분류 문제란 무엇인가

주어진 데이터를 두 개의 카테고리로 분류하는 문제로 어제 하였단 and or not xor 이나 스팸메일 필터예시가 있다

### 문제 해결을 위한 과정
1. 데이터 수집
    - 분류할 데이터를 준비 (문제 데이터와 정답 데이터)
2. 데이터 전처리
    - 결측값 제거및 데이터 정규화/ 표준화
    - 범주형 데이터 변환
3. 모델 선택
    - 이진 분류에 적합한 머신러닝 모델 선택
        - 로지스틱 회귀(Logistic Regression)
        - 의사결정 나무(Decision Tree)
        - 랜덤 포레스트(Random Forest)
        - 서포트 벡터 머신(SVM)
        - 신경망(Neural Network, 딥러닝)
4. 모델 학습
    - 학습데이터와 테스트 데이터로 나누어 학습을 진행
5. 모델 평가
    - 모델의 성능을 측정하는 평가 지표 활용
        - 정확도(Accuracy): 전체 샘플 중 올바르게 분류된 비율
        - 정밀도(Precision): 1로 예측한 것 중 실제 1인 비율
        - 재현율(Recall): 실제 1인 것 중 1로 예측한 비율
        - F1-score: 정밀도와 재현율의 조화 평균
        - ROC-AUC: 모델의 판별 능력 평가
6. 모델 배포및 활용

#### Question
1. 로지스틱 회귀, 의사결정 나무, 랜덤 포레스트 이것을은 무엇인가
2. 정밀도와 재현율은 혼돈행렬과 관련이 있을까?

#### Anser
Q1
1. 로지스틱 회귀(Logistic Regression)
    ![이지미](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FADJ4A%2Fbtq3UGEHoJK%2FsHY3A2hITOUEkvLDhhqfa1%2Fimg.png)
- 개념: 선형 회귀와 비슷하지만, 결과를 0~1 사이의 확률로 변환하는 알고리즘
- 사용 예시: 이메일이 스팸인지 아닌지, 고객이 상품을 구매할지 안 할지 예측
- 작동 원리: 
    - 입력 데이터에 가중치를 곱해 합을 구함 
    - 시그모이드(𝜎) 함수를 적용해 값을 0~1 사이의 확률로 변환 
    - 임계값(보통 0.5)보다 크면 1, 작으면 0으로 분류
- 장점: 간단하고 해석이 쉬움
- 단점: 복잡한 패턴을 학습하는 데 한계가 있음
2. 의사결정 나무(Decision Tree)
    ![이미지](https://wikidocs.net/images/page/39491/%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4.png)
- 개념: 데이터를 여러 개의 질문(조건)으로 나누어가며 분류하는 트리 구조의 알고리즘
- 사용 예시: 대출 신청자가 돈을 갚을 수 있는지(승인/거절), 질병 진단(환자/정상)
- 작동 원리:
    - 데이터를 여러 개의 분기(질문)로 나누어가며 최적의 분할 지점을 찾음
    - 각 노드에서 특정 기준(예: 나이 > 30?)을 통해 데이터를 분리
    - 최종적으로 리프(끝 노드)에 도달하면 해당 클래스로 분류
- 장점: 직관적이며 사람이 이해하기 쉬움
- 단점: 트리가 너무 깊어지면 과적합(overfitting) 위험이 있음
3. 랜덤 포레스트(Random Forest)
    ![이지미](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fc31hH0%2FbtrvkHD8ihe%2F237tPyencFSIsjMXnUZ19k%2Fimg.png)
- 개념: 여러 개의 의사결정 나무를 조합하여 더 강력한 예측을 수행하는 앙상블 학습 기법
- 사용 예시: 고객이 이탈할지 예측, 사기 거래 탐지, 의료 진단
- 작동 원리:
    - 여러 개의 의사결정 나무를 랜덤하게 학습
    - 각 나무에서 독립적으로 예측을 수행한 후, 투표(majority voting) 또는 평균을 내어 최종 결과 결정
- 장점: 과적합을 방지하고 높은 정확도를 가짐
- 단점: 개별 나무보다 학습 속도가 느리고 해석이 어려울 수 있음

Q2
정밀도(Precision)와 재현율(Recall)는 혼돈 행렬(Confusion Matrix)에서 직접 계산되며, 모델의 성능을 평가하는 핵심 지표이다

1. 혼돈 행렬(Confusion Matrix)이란?
이진 분류 모델의 예측 결과를 **실제값과 비교하여 정리한 표**입니다.  


|                | **실제 양성(1)** | **실제 음성(0)** |
|--------------|----------------|----------------|
| **예측 양성(1)** | **TP (참 양성, True Positive)** | **FP (거짓 양성, False Positive)** |
| **예측 음성(0)** | **FN (거짓 음성, False Negative)** | **TN (참 음성, True Negative)** |



    - **TP (참 양성)**: 실제 1인데 1로 맞게 예측  
    - **FP (거짓 양성)**: 실제 0인데 1로 잘못 예측 (False Alarm)  
    - **FN (거짓 음성)**: 실제 1인데 0으로 잘못 예측 (놓침)  
    - **TN (참 음성)**: 실제 0인데 0으로 맞게 예측  

---

2. 정밀도(Precision)란?
**모델이 1(양성)이라고 예측한 것 중, 실제로 1인 비율**  

    $$
    \text{Precision} = \frac{TP}{TP + FP}
    $$

    - **높을수록 FP(거짓 양성)를 줄임 → 잘못된 경고(False Alarm) 감소**  
    - 예시: 스팸 메일 필터에서 **정밀도가 높으면** 실제 스팸만 걸러내고 정상 메일을 스팸으로 분류하는 실수를 줄임  

---

3. 재현율(Recall)란?
**실제로 1(양성)인 것 중에서, 모델이 1로 잘 예측한 비율**  

    $$
    \text{Recall} = \frac{TP}{TP + FN}
    $$

    - **높을수록 FN(거짓 음성)을 줄임 → 중요한 걸 놓치는 실수(False Negative) 감소**  
    - 예시: 암 진단 모델에서 **재현율이 높으면** 실제 암 환자를 놓치지 않고 잘 찾아냄  

